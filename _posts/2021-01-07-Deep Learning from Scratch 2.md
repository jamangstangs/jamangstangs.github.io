---

title: Deep Learning from Scratch 2

toc: true

use_math: true

categories:

 - Deep Learning

---

## 자연어와 단어의 분산표현

자연어 처리가 다루는 분야는 다양하지만, 그 본질은 **컴퓨터가 우리의 말을 이해하게 하는 것이다.** 이번 챕터에서 이해해야 할 목표는 아래와 같다.

- 컴퓨터가 말을 이해한다는 말이 무엇인가? (딥러닝 등장 이전 기법도 살펴볼 것이다.)
- 파이썬으로 텍스트를 다루는 방법

### 자연어 처리란

자연어 처리는 Natural Language Processing(LNP)라고 한다. 자연어 처리에서 추구하는 목표는 사람의 말을 컴퓨터가 이해하도록 만들어서, 컴퓨터가 우리에게 도움이 되는 일을 수행하게 만드는 것이다. 하지만, 컴퓨터는 사람의 말을 곧이 곧대로 이해하는 것이 아닌 마크업, 혹은 프로그래밍 언어로 이해한다고 생각할 것이다. 컴퓨터는 일관되고 한 글자라도 틀리면 이해를 하지 못하는 프로그래밍 언어를 이해하는데, 다양한 표현과 어느정도 문법이 어긋나도 이해가 가능한 부드러운 언어(인간의 언어)를 어떻게 이해시킬까? 이 어려움을 해결해 나가는 것이 우리의 목적이라고 생각하면 된다. 

### 단어의 의미

인간의 말은 **문자**로 구성되어 있으며 말의 의미는 **단어**로 구성이 된다. 단어는 다시 말해서 **의미의 최소 단위**라고 말할 수 있다. 따라서 자연어를 컴퓨터에게 이해시키는데는 무엇보다 **단어의 의미**를 이해시키는 것이 중요하다. 

이번 주제에서는 컴퓨터에게 **단어의 의미를 이해시키는 것**이다. 단어의 의미를 잘 파악하는 표현 방법에 대해서는 아래 세 가지 방법에서 추론할 수 있다.

- 시소러스를 활용 : 시소러스(유의어 사전)을 이용한다.
- 통계 기반 기법 : 통계 정보로부터 단어를 표현하는 기법
- 추론 기반 기법 : word2vec을 다루는 기법

## 시소러스 기법

시소러스는 기본적으로 유의어 사전으로 뜻이 같은 단어(동의어)나 뜻이 비슷한 던어(유의어)가 한 그룹으로 분류되어 있다. 또한 자연어 처리에 사용되는 시소러스에서는 단어 사이의 **상위 또는 하위** 관계까지 세세하게 정의해둔 경우가 있다. **유의어, 동의어, 상위 또는 하위 관계** 를 알고있다면, 즉 **단어 네트워크** 를 컴퓨터가 이해한다면 단어의 의미를 이해시켰다고 어쩌면 주장이 가능할 수도 있다. 따라서 이러한 지식들을 이용하여 우리에게 유용한 일들(번역, 챗봇)과 같은 유용한 일들을 컴퓨터에게 시킬 수 있다. 

**WordNet** : 자연어 처리분야에서 가장 유명한 시소러스이며, 많은 연구와 다양한 자연어 처리 애플리케이션에서 활용되고 있다. 이것을 사용하면 유의어를 얻거나 **단어 네트워크** 를 사용해 단어 사이의 **유사도** 를 구할 수 있다. 

**WordNet의 문제점** : 시대에 따라 단어의 의미가 변할 수 있다. 신조어 같은 단어들이 생겨나고, 기존의 단어들의 의미가 미래에는 어쩌면 또 다른 의미로 쓰일 수 있기 때문에 사람이 수작업 하는 방식의 시소러스는 끊임없이 업데이트를 해야한다. 하지만 사람들 사용하여 업데이트 하는데에는 비용이 너무 크다. 또한 유이어 단어의 미묘한 차이를 무시하고 같은 클래스로 묶어내므로 미묘한 표현을 할 수가 없다.

=> 이러한 문제를 피하기 위해 **통계 기반 기법** 과 **추론 기반 기법** 이 존재하는데, 이 둘의 기법에서는 **단어의 의미를 자동으로 추출한다.** 따라서 수작업으로 수집되는 시소러스와는 달리 엄청난 효율을 자랑한다.

## 통계 기반 기법

통계 기반 기법에서는 **말뭉치(corpus)** 를 사용할 것이다. 말뭉치는 단순히 말해 대량의 텍스트 데이터이다. 다만 자연어 처리를 염두해두고 수집된 텍스트 데이터를 의미한다. 결국 말뭉치에는 사람의 지식이 충분히 담겨있다고 생각하며 자동으로 핵심을 추출해내는 기법이다. 추가로 말뭉치에 추가 정보가 포함되는 경우도 있다. 이제 통계 기반 기법을 사용하기 위한 스텝을 밟아 볼 것이다.

### 파이썬으로 말뭉치 전처리하기

여기서 말하는 전처리는 텍스트 데이터를 단어로 분할하고, 분할된 단어들을 ID로 변환하는 작업을 말한는 것이다. 그렇다면 하나식 확인해가면서 단계별로 진행해보자. 

- 텍스트 분할하기

```python
>>> text = 'You said goodbye and say hello.'
# 위와 같이 텍스트를 만들어 보자. 실전에서는 위의 텍스트보다 엄청 긴 텍스트일 것이다.
>>> text = text.lower()
# 모든 글자를 소문자로 변환
>>> text = text.replace('.',' .')
# 뒤의 공백을 기준으로 텍스트를 분할하므로 '.'을 공백을 포함한 ' .'으로 바꾼다.
>>> words = text.split(' ')
# 공백을 기준으로 나눈다. 이때 words는 리스트이다.
>>> words
['You', 'said', 'goodbye', 'and', 'I', 'say', 'Hello.']
```

위의 과정에 따라서 문장에서 단어를 분할하여 목록의 형태로 만들었지만, 단어를 텍스트 그대로 조작하기는 매우 어렵다. 따라서 단어에 ID를 부여하고 ID의 리스트로 이용할 수 있도록 다시 손질을 한다.

- 단어ID to 단어 & 단어 to 단어ID

```python
>>> word2id = {}
>>> id2word = {}
>>>
>>> for word in words:
  			if word not in word2id:
      		newid = len(word2id)
        	word2id[word] = newid
          id2word[newid] = word
>>> id2word
{0: 'You', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'I', 5: 'say', 6: 'Hello.'}
>>> word2id
{'You': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'I': 4, 'say': 5, 'Hello.': 6}
>>>corpus = np.array([word2id[w] for w in words])
>>> corpus 
array([0,1,2,3,4,1,5,6])
```

- id2word : 단어ID에서 단어로의 변환을 담당하며, 단어ID가 key, 단어가 value이다.
- word2id : 단어에서 단어ID로의 변환을 담당하며, 단어가 key, 단어ID가 value이다.
- newdi : 추가 시점에서 현재 딕셔너리의 길이가 ID의 값이 된다.
- corpus : 단어 ID 목록이며, text에 그대로 대응하는 ID값이다.

### 단어의 분산표현

색을 "비색"과 같이 표현하는 것 보다 RGB = (255,0,0)과 같이 표현한다면 빨강 계열의 색임을 알 수 있다. 색을 이와같이 벡터로 표현한다면 좀 더 쉽게 판단할 수 있고 정량화 하기도 쉽다. 그렇다면 단어도 벡터로 표현하면 어떻까? 좀 더 자세히 말하면 벡터라는 표현을 **단어에도 구축할 수 있을까?** 이제부터 알아볼 것은 **단어의 의미** 를 정확하게 파악할 수 있는 **벡터표현** 이다. 

- 색의 벡터표현을 **RGB표현**이라고 하듯이, 단어의 벡터표현을 **분산 표현(Distributional Represntation)**이라고 한다.

### 분포가설 

자연어 처리의 역사에서 오랜 연구가 이뤄져 왔지만, 가장 기본적인 아이디어 하나를 근간으로 한다. 그 아이디어는 **단어의 의미는 주변 단어에 의해 형성이 된다**는 것이다. 이를 **분포가설**이라고 하며 최근의 연구도 이를 기반으로 한다. 

- 분포 가설 : 단어 자체에는 의미가 없고 그 단어가 사용된 맥락이 의미를 형성한다는 것이다. 후에 **맥락**이라는 말을 많이 사용하는데, 맥락이라 함은 주목하는 단어 주위에 놓인 단어를 가리킨다고 하자.
- you say **goodbye** and i say hello.  -> 여기서 goodbye를 주목한다고 하면 그 주변의 단어 say, and와 같은 것들이 맥락이며, 맥락의 크기를 **윈도우 크기(Window Size)**라고 한다.

### 동시발생 행렬

분포가설을 생각해보자. 주목하는 단어 주위의 어떤 단어가 맥락에 포함되는지 알아보고 싶다면, 우리는 주변 단어를 '세어보는'방법을 생각해낼 것이다. 즉, 어떤 단어를 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법이다. 이를 앞서 우리 중간 제목의 주제인 **통계 기반 기법(Statistical Method)**이라고 한다. 일단 전처리를 하는 작업을 거치고 난 뒤에 주목하는 단어의 맥락을 벡터의 형태로 표현해보는 절차를 밟아보겠다.

```python
import sys
sys.path.append('..')
import numpy as np
from common.util import preprocess

text = 'You say hello and i say hello.'
corpus, word2id, id2word = preprocess(text)

print(corpus)
# [0,1,2,3,4,1,5,6]

print(id2word)
{0: 'You', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'I', 5: 'say', 6: 'Hello.'}
```

- 여기서는 단어의 수가 총 7개임을 알 수 있고, 각 단어마다 맥락에 포함되는 단어를 표의 형태로 나타내보겠다. (윈도우 크기는 1이다.)

|         | you  | say  | goodbye | and  | i    | hello | .    |
| ------- | ---- | ---- | ------- | ---- | ---- | ----- | ---- |
| you     | 0    | 1    | 0       | 0    | 0    | 0     | 0    |
| say     | 1    | 0    | 1       | 0    | 1    | 1     | 0    |
| goodbye | 0    | 1    | 0       | 1    | 0    | 0     | 0    |
| and     | 0    | 0    | 1       | 0    | 1    | 0     | 0    |
| i       | 0    | 1    | 0       | 1    | 0    | 0     | 0    |
| hello   | 0    | 1    | 0       | 0    | 0    | 0     | 1    |
| .       | 0    | 0    | 0       | 0    | 0    | 1     | 0    |

이 표가 행렬의 형태를 띤다는 뜻에서 **동시발생 행렬(Co-occurence matrix)**라고 한다. 이와 같은 행렬을 C라는 이름의 배열에 저장했다고 하면, and를 나타내는 배열을 C[3] 벡터로 나타낼 수 있다. 아래에서는 동시발생 행렬을 만들어주는 함수를 구현해보겠다.

```python
def create_co_matrix(corpus, vocab_size, windwo_size = 1):
  corpus_size = len(corpus)	# 단어 id의 리스트
  co_matrix = np.zeros((vocab_size,vocab_size),dtype = np.int32)
  
  for idx, word_id in enumerate(corpus):
    for i in range(1, window_size +1):
      left_idx = idx-i
      rigth_idx = idx+i
      
      if left_idx >=0:
        left_word_id = corpus[left_idx]
        co_matrix[word_id, left_word_id] +=1
       
      if right_idx <=corpus_size :
        right_word_id = corpus[right_idx]
        co_matrix[word_id, right_word_id] +=1
        
  return co_matrix
```

- 위와 같이 차근차근 구현하면 말뭉치를 동시발생 행렬로 만들어주는 함수를 구현할 수 있다. 또한 구현할 시 윈도우의 크기가 말뭉치의 크기를 벗어나지 않도록 조건을 달아주자.

### 벡터 간 유사도 

위에서는 동시발생 행렬을 활용하여 단어를 벡터로 표현하는 방법을 알아보았다. 그렇다면 여기서 **벡터 사이의 유사도**를 측정하는 방법을 살펴보자. 보통 단어벡터의 유사도를 나타낼때는 **코사인 유사도**를 자주 이용한다. 코사인 유사도는 다음과 같이 정의된다.

​		      $  similarity(\mathbf{x},\mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}} {\Vert \mathbf{x} \Vert \Vert \mathbf{y} \Vert}  = \frac{x_1y_1 + \cdot \cdot \cdot + x_n y_n}{\sqrt{x_1^2 + \cdot \cdot \cdot + x_n^2} \sqrt{y_1^2 + \cdot \cdot \cdot + y_n^2}}$

-  분자에는 벡터의 내적이, 분모에는 각 벡터의 Norm이 등장한다. 노름(norm)은 벡터의 크기를 나타낸 것으로, 각 원소를 제곱해서 더한 후 다시 제곱근을 구해 계산하는 것이다. 이 식을 직관적으로 이해하면 두 벡터가 가리키는 방향이 얼마나 비슷한가이며, 완전히 같다면 1, 완전히 다르다면 -1의 값을 지니게 된다. 위의 식을 파이썬으로 구현하면 다음과 같다.

```python
def cos_similarity(x,y,eps = 1e-8):
  nx = x/np.sqrt(np.sum(x**2)+eps)
  ny = y/np.sqrt(np.su,(y**2)+eps)
  return np.dot(nx,ny)
```

- 여기서 이 함수는 벡터를 먼저 정규화 하고 dot product를 하였다. 여기서 벡터의 정규화는 단위 벡터를 만든다는 의미이며, 방향 정보만이 남게된다. 여기서 dot product를 한다면 방향정보간의 계산이 되므로 방향이 같다면 1, 완전히 다르다면 -1이라는 값이 나오게 된다. 이와 같은 성질 때문에 cos_similarity라는 이름이 붙게 되었다. 
- eps : epsilon의 약자이며, 분모가 0이되면 오류가 발생하므로 분모에 작은 값을 더해준다.

이제 코사인 유사도를 사용하여 위의 "i" 와 "you"의 유사도를 구해보겠다.

```python
import sys
sys.path.append('..')
from common.util import preprocess, create_co_matrix, cos_similarity

text = "you say goodbye and i say hello."
corpus, word2id, id2word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus,vocab_size)

c0 = C[word2id['you']]
c1 = C[word2id['i']]
pruint(cos_similarity(c0,c1))
```









