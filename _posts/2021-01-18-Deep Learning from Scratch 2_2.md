---

title: Deep Learning from Scratch 2-2

toc: true

use_math: true

categories:

 - Deep Learning

---



지금까지 내가 구현해왔던 신경망들은 피드포워드(Feed Forward)라는 유형의 신경망으로, 흐름이 단반향인 신경망을 의미한다. 즉, 입력 신호가 다음 층으로 또 다음 층으로 전달되고.. 이렇게 한 방향으로만 전달되는 신경망을 의미한다.

하지만, 피드포워드 유형의 신경망은 **시계열 데이터**를 잘 다루지 못한다는게 단점으로 작용한다. 자세히 말하면 피드포워드 유형의 신경망에서는 시계열 데이터의 **성질(패턴)**을 충분히 학습할 수가 없다고 한다. 따라서 **순환 신경망(RNN)**이 등장하게 된다. 

이제부터 알아낼 두 가지 정보는 아래와 같다.

- 피드포워드 신경망의 문제점을 알아보자.
- RNN이 문제점을 어떻게 해결하는가

## 확률과 언어모델

지금까지 배운 word2vec의 CBOW모델을 복습해보겠다. 단어열을 나열 할 때, $w_t$은 타깃,  $w_{t-1}, w_{t+1}$은 타깃 주변의 맥락이라고 표현하였다. 그러면 맥락이 주어졌을 때 타깃이 $W_t$가 될 확률은 아래와 같이 나타낸다.

<center>
  $P(w_{t} \mid w_{t-1},w_{t+1})$
</center>

- 이는 윈도우 크기가 1일 때의 CBOW을 모델을 나타낸 사후 확률 모델링이며, 맥락이 좌우 대칭일 때를 나타낸 확률이다.

하지만, 우리는 이제 맥락을 왼쪽 윈도우 한정으로 나타낸다면 확률 모델 표기는 아래와 같다.

<center>
  $P(w_t \mid w_{t-2}, w_{t-1})$
</center>

- 맥락이 왼쪽 두 단어만을 의미하는 확률 모델이다.

<center>
  $L = -logP(w_t \mid w_{t-2}, w_{t-1}$
</center>

- 위의 확률 모델을 사용해 손실 함수를 나타낸 수식이다. 

우리의 목적은 손실 함수를 최소화 하는 가중치 매개변수를 찾는 것이다. 이러한 가중치 매개변수가 발견되면 CBOW모델로 부터 맥락이 주어졌을 때 타깃을 정확하게 추출할 수 있을 것이다. 이러한 목적을 가지고 학습을 진행할 때 단어의 의미가 인코딩 된 **분산표현**을 얻을 수 있는 것이다.

그렇다면, 우리가 주목해야 할 것은 분산표현을 얻는 것이 아닌 원래 목적인 **맥락으로 부터 타깃을 추측하는 능력**은 어디에 써야하는 것인가? 여기서 우리는 **언어모델**을 기억하고 다음 장으로 넘어가도록 하자.

**우리가 가져가야 할 궁금증**: 윈도우는 하이퍼 파라미터이며, 우리가 정하는 값인데 윈도우의 크기를 좌우 비대칭으로 설정한 이유는 도대체 무엇일까?

### 언어모델

언어 모델은 수식에서 알 수 있듯이 **단어 나열에 확률을 부여**한다.  즉, "you say goodbye" 라는 시퀀스에는 높은 확률을 부여하고 "you say good die"라는 시퀀스에는 낮은 확률을 부여하는 것이 언어 모델이라고 할 수 있다.

그렇다면 특정한 단어 시퀀스에 확률을 부여하는 것을 사용하여 어떤 것을 할 수 있는지 알아보자.

- 음성 인식 시스템 : 사람의 음성으로부터 몇 개의 문장을 후보로 생성한다. 그 다음 후보 문장이 문장으로써 자연스러운지 순서를 매길 수 있다.
- 새로운 문장 생성 : 단어 순서의 자연스러움을 확률 적으로 평가가 가능하므로 다음으로 적합한 단어를 **확률에 따라 샘플링이 가능하다.**

#### 확률의 곱셈정리

<center>
  $P(A,B) = P(A \mid B)P(B)
</center>

- P(A,B): A랑 B 모두 일어날 확률을 의미한다. 또한 이때 사후 확률에서 조건의 순서를 바꿔도 값은 똑같이 나온다. 

#### 확률의 곱셈정리를 이용한 언어모델의 수식

<center>
  $P(w_1,...,w_{m}) = P(w_m\mid w_1,...,w_{m-1})P(w_{m-1}\mid w_1,...,w_{m-2}) \cdot \cdot \cdot P(w_3\mid w_2, w_1)P(w_2\mid w_1) P(w_1) = \Pi_t=1^m P(w_t \mid w_1,...,w_{t-1})$
</center>

- m 개의 단어의 동시에 나타날 확률을 사후 확률로 나타낸 것이며, 간단하게 축약하여 표현하면 아래와 같다.

<center>
 $ P(w_1,...,w_{m-1},w_m) = P(A,w_m) = P(w_m \mid A)P(A)$
</center>

- 이와 같이 단어 1개씩 떼가며 사후 확률로 표현하면 기존에 $\Pi$로 표현하던 식과 같이 나오게 된다. 

지금까지 써왔던 수식들에서 주목해야 할 점은 사후 확률이 타깃 단어보다 왼쪽에 있는 모든 단어들을 **맥락** 으로 했을 때의 확률인 것이다. 

### CBOW모델을 언어모델로 

우리가 실제로 구현할 때, 맥락의 크기는 제한적이다. 따라서 맥락의 크기를 말뭉치 전체에 적용하여 하는 것은 상당히 무리가 있으며, 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타내야한다. 수식으로는 아래와 같다.

<center>
  $ P(w_1,...,w_m) = \Pi^m_{t=1} P(w_t \mid w_1,...,w_{t-1}) \approx \Pi^m_{t=1} P(w_t \mid w_{t-2},w_{t-1})
</center>

- 근사한 식은 맥락을 왼쪽 2개의 단어로 한정한다. 따라서 CBOW 모델의 맥락의 크기에 따라 근사적으로 나타낼 수 있음을 알 수 있다.

맥락의 크기는 임의의 길이로 **설정** 하였다는 의미는 특정 길이로 **고정** 이 된다는 의미이다. 이는 현재 나와야 하는 단어가 맥락의 크기를 벗어난 곳에 위치해 있다면 확률모델을 적용할 수 없어 답할수가 없다. 따라서 우리는 **맥락의 크기가 코정됨을 알고 있어야 한다.**

그렇다면 맥락의 크기만 키우면 문제가 해결이 되는가? 아니다. CBOW 모델에서는 맥락 안의 단어 순서가 **무시 된다는 한계가 있다.**



